%\section{Experiment}\label{sec:experiment}
\section{Empirical results} \label{sec:experiment}
To measure the efficiency of our approach, we conducted two empirical studies. The first one takes into consideration the whole system 
(PEPs and PDPs) to evaluate the performance improvement regarding the decision making process. The request processing time, 
for each splitting criterion is compared to the processing time of the initial architecture implementing the global policy (the evaluation 
that considers the global policy, is denoted (IA), the ``Initial Architecture''). The second empirical study focuses only on the PDPs in isolation to measure the gain in performance
 independently from the system. To make such study of PDPs in isolation, we use XEngine \cite{Xengine}. The objective of the second study is to see how our approach can be 
combined with XEngine and how this impacts the performance. The first subsection introduces our empirical studies and presents the tool that supports our approach. The remaining two 
sections present and discuss the results of the two empirical studies.  

\subsection{Empirical Studies and PolicySplitter Tool}
The empirical studies were conducted using the following systems \cite{testcase}:
\begin{itemize}	
\item LMS: The library management system offers services to manage books in a public library.
\item VMS: The virtual meeting system offers simplified web conference services. The virtual meeting server allows the organization of work meetings on 
a distributed platform.
\item ASMS (Auction Sale Management System): allows users to buy or sell items online. A seller can start an auction by submitting a description of the
item he wants to sell and a minimum price (with a start date and an ending date for the auction). Then usual bidding process can apply and people can bid 
on this auction. One of the specificities of this system is that a buyer must have enough money in his account before bidding.
\end{itemize}
We completed the set of existing rules with all implicit rules that were not explicitely originally described. This provides a larger set of access rules that enables
an accurate comparison of performances. As a result, LMS policy contains 720 rules, VMS has 945 rules while ASMS implements 1760 rules. 
Our evaluations were carried out on a desktop PC, running Ubuntu 10.04 with Core i5, 2530 Mhz processor, and 4 GB of RAM. 
We have implemented a tool to automate policies refactoring. PolicySplitter enables splitting the policies according to the chosen splitting criteria. 
The tool is implemented using the Java language and is available for download from \cite{splitter}.
The execution time of the tool is not considered as a performance factor as it takes up to few seconds (for very large policies) to perform the splitting 
according to all SCs. Moreover the refactoring process is executed only once to create a configuration that supports a selected splitting criterion.


\subsection{Performance Improvement Results}
For the 3 evaluation studies, we generated the resulting sub-policies for all the splitting criteria that we have defined in Section III.
The decision Engine in our three case studies is based on Sun XACML PDP implementation \cite{sunxacml}. We choose to use Sun XACML PDP instead of XEngine in order 
to prove the effectiveness of our approach when compared to the traditional architecture.
% To run the experiments, we used a configuartion file that maintains the list of policies used for requests evaluation. 
% For the initial architecture, the configuration file contains one policy. Sun XACML PDP uses this configuration file to select to XACML files that constitute the policy.
For each splitting criteria, we have conducted systems tests to generate requests that trigger all the PEPs in the three evaluation studies. 
The test generation step leads to the execution of all combination of possible requests, all the details related to this step are presented 
in details in our previous \cite{testcase}.  
The process of tests generation is repeated ten times in order to avoid the impact of randomness.
We applied this process to each splitting criterion and calculated the average execution time of the system under tests.


% We only consider the execution time of the PDP and we do not include the executions of the system functions. 
The results are shown in Figure \ref{fig:processing time}. They show the execution time considering the sub-policies resulting from each splitting criterion and the global policy 
that corresponds to the initial architecture (IA). Note that the results are ranked from the largest processing duration to the smallest one. 
Through the results shown in Figure \ref{fig:processing time}, we can make two observations:

\begin{itemize} 
\item Compared to the initial architecture (IA), the evaluation time is considerably reduced for all the splitting criteria. This is consistent with our 
initial intuition. In fact, splitting the policy into small policies improves requests processing duration.


\item The splitting criteria \normalsize $SC=\langle Action, Resource\rangle$ enables to have the best evaluation time. 
recall that the PEPs in the 3 empirical studies are scattered across the applications by a categorization 
that is based on $SC_{2}=\langle Resource,Action\rangle$. This pleads in favor of applying a splitting criteria 
that takes into account the PEP-PDP synergy. This splitting preserves the initial architecture by keeping a strog mapping between the PEPS and the PDP and enables 
to have the best results in terms of performance.

\end{itemize} 
\begin{figure*}
  \centering
  \subfloat[LMS]{\label{fig:gull}\includegraphics[width=0.33\textwidth]{LMS.pdf}}                
  \subfloat[VMS]{\label{fig:VMS}\includegraphics[width=0.33\textwidth]{VMS.pdf}}
  \subfloat[ASMS]{\label{fig:ASMS}\includegraphics[width=0.33\textwidth]{ASMS.pdf}}
  \caption{Processing Time for our 3 systems LMS, VMS and ASMS}
  \label{fig:processing time}
\end{figure*}
\begin{figure}[!h]
  \centering
\includegraphics[width=8.5cm, height=7.2cm]{pdpnumber.pdf}
\begin{center}
\caption{PDP Number According to Splitting Criteria}
\label{pdpnumber}
\end{center}
\end{figure} 

We have evaluated the PDPs number generated by each splitting criterion, to study the impact of the refactoring process on the initial policy. 
For the 3 XACML studies, we executed the PoliySplitter tool on the 3 initial policies and we generated the number of resulting policies, in each study. 
As highlighted by Figure \ref{pdpnumber}, we notice that there are three
 categories of results: $SC_{1}$ category leads to a small number of PDPs, $SC_{2}$ category produces a reasonable number of PDPs whereas $SC_{3}$ leads to a huge 
number of PDPs. $SC_{2}$ category, is a good tradeoff in terms of performance and number of 
PDPs generated: In our evaluation studies, $SC=\langle Action, Resource\rangle$ is the best criterion, both in terms of performances and low number of PDPs.


\begin{table}[h!]
\centering
\begin{tabular}{|>{\tiny}c|>{\tiny}c|>{\tiny}c|>{\tiny}c|>{\tiny}c|>{\tiny}c|>{\tiny}c|>{\tiny}c|>{\tiny}c|}   
\hline  \rowcolor{black} \scriptsize \bf \textcolor {white}{}
& \scriptsize \bf \textcolor {white}{S}
& \scriptsize \bf \textcolor {white}{A}
& \scriptsize \bf \textcolor  {white}{R}
& \scriptsize \bf \textcolor  {white}{SA}
& \scriptsize \bf \textcolor  {white}{SR}
& \scriptsize \bf \textcolor  {white}{AR} 
& \scriptsize \bf \textcolor  {white}{SAR}
& \scriptsize \bf \textcolor {white}{IA}\\ \hline


\scriptsize  {LMS}
&\scriptsize  {120}
& \scriptsize {72}
& \scriptsize {240}
& \scriptsize {12}
& \scriptsize {40}
& \scriptsize {24}
& \scriptsize {4}
& \scriptsize {720}
  \\ \hline


\scriptsize  {VMS}
&\scriptsize  {135}
& \scriptsize {63}
& \scriptsize {315}
& \scriptsize {9}
& \scriptsize {45}
& \scriptsize {21}
& \scriptsize {3}
& \scriptsize {945}
  \\ \hline
\scriptsize  {ASMS}
&\scriptsize  {220}
& \scriptsize {160}
& \scriptsize {352}
& \scriptsize {20}
& \scriptsize {44}
& \scriptsize {32}
& \scriptsize {4}
& \scriptsize {1760}
  \\ \hline
\scriptsize  {Synergic}
&\scriptsize  {}
& \scriptsize {x}
& \scriptsize {x}
& \scriptsize {}
& \scriptsize {}
& \scriptsize {x}
& \scriptsize {}
& \scriptsize {x}
  \\ \hline

\scriptsize  {Not Synergic}
&\scriptsize  {x}
& \scriptsize {}
& \scriptsize {}
& \scriptsize {x}
& \scriptsize {x}
& \scriptsize {}
& \scriptsize {x}
& \scriptsize {}
  \\ \hline
\end{tabular}
\caption{Average of rules number/PDP in the 3 systems}\end{table}

\subsubsection{Performance improvement with XEngine}
The goal of this empirical study is to show the impact of combining XEngine as a decision engine rather than Sun XACML PDP implementation with our approach. 
In this step, we show the effectiveness of the refactoring process independently from the appication code.
We have chosen to use \cite{Xengine}, mainly for 3 reasons:

\begin{itemize}
\item It uses a refactoring process that transforms the hierarchical structure of the XACML policy to a flat structure. 
\item It converts multiple combining algorithms to single one.
\item It lies on a tree structure that minimizes the request processing time.
\end{itemize}
We propose to use XEngine conjointly with the refactoring process presented in this work:
We have evaluated our approach in 2 settings:
\begin{itemize}
\item Considering evaluations with a decision engine, based on SUN PDP, with split policies and with the initial policy.  
\item Considering evaluations with a decision engine, based on XEngine rather than Sun PDP, with split policies and with the initial policy as well.  
\end{itemize}

This step consists in two parts, first, we consider to measure the request evaluation time for a fixed number of request, then we measure the request evaluation
 time by taking into consideration the load of the overall system.


- Using XEngine with a fixed number of requests
We have measured the processing time (in ms) of a randomly generated set of 10,000 requests. For request generation, we have used the technique presented 
in \cite{request}. The request time processing is evaluated for LMS, VMS, ASMS. The results are presented in Table I, II and III.

\begin{table}[h!]
\centering
\begin{tabular}{|>{\tiny}c|>{\tiny}c|>{\tiny}c|>{\tiny}c|>{\tiny}c|>{\tiny}c|>{\tiny}c|>{\tiny}c|>{\tiny}c|}   
\hline  \rowcolor{black} \scriptsize \bf \textcolor {white}{}
& \scriptsize \bf \textcolor {white}{SAR}
& \scriptsize \bf \textcolor {white}{AR}
& \scriptsize \bf \textcolor  {white}{SA}
& \scriptsize \bf \textcolor  {white}{SR}
& \scriptsize \bf \textcolor  {white}{R}

& \scriptsize \bf \textcolor  {white}{S} 
& \scriptsize \bf \textcolor  {white}{A}
& \scriptsize \bf \textcolor {white}{IA}\\ \hline
\scriptsize  {SUN }
&\scriptsize  {485}
& \scriptsize {922}
& \scriptsize {1453}
& \scriptsize {1875}
& \scriptsize {2578}

& \scriptsize {2703}
& \scriptsize {2703}
& \scriptsize {2625}
  \\ \hline
\scriptsize  {XEn}
&\scriptsize  {26}
& \scriptsize {47}
& \scriptsize {67}
& \scriptsize {95}
& \scriptsize {190}

& \scriptsize {164}
& \scriptsize {120}
& \scriptsize {613}
  \\ \hline
\end{tabular}
\caption{Evaluation time in LMS}\end{table}



\begin{table}[h!]
\centering
\begin{tabular}{|l|l|l|l|l|l|l|l|l|}   
\hline  \rowcolor{black} \scriptsize \bf \textcolor {white}{}
& \scriptsize \bf \textcolor {white}{SAR}
& \scriptsize \bf \textcolor {white}{AR}
& \scriptsize \bf \textcolor  {white}{SA}
& \scriptsize \bf \textcolor  {white}{SR}
& \scriptsize \bf \textcolor  {white}{R}

& \scriptsize \bf \textcolor  {white}{S} 
& \scriptsize \bf \textcolor  {white}{A}

& \scriptsize \bf \textcolor {white}{IA}\\ \hline

\scriptsize  {SUN }
& \scriptsize  {1281}
& \scriptsize {2640}
& \scriptsize {3422}
& \scriptsize {3734}
& \scriptsize {6078}

& \scriptsize {5921}
& \scriptsize {6781}
& \scriptsize {5766}
  \\ \hline
\scriptsize  {XEn}
& \scriptsize  {34}
& \scriptsize {67}
& \scriptsize {96}
& \scriptsize {145}
& \scriptsize {384}

& \scriptsize {274}
& \scriptsize {149}
& \scriptsize {265}
  \\ \hline
\end{tabular}
\caption{Evaluation time in VMS}\end{table}

\begin{table}[h!]
\centering
\begin{tabular}{|l|l|l|l|l|l|l|l|l|}   
\hline  \rowcolor{black} \scriptsize \bf \textcolor {white}{}
& \scriptsize \bf \textcolor {white}{SAR}
& \scriptsize \bf \textcolor {white}{AR}
& \scriptsize \bf \textcolor  {white}{SA}
& \scriptsize \bf \textcolor  {white}{SR}
& \scriptsize \bf \textcolor  {white}{R}

& \scriptsize \bf \textcolor  {white}{S} 
& \scriptsize \bf \textcolor  {white}{A}
& \scriptsize \bf \textcolor {white}{IA}\\ \hline
\scriptsize  {SUN }
& \scriptsize  {2280}
& \scriptsize {2734}
& \scriptsize {3625}
& \scriptsize {8297}
& \scriptsize {7750}

& \scriptsize {8188}
& \scriptsize {6859}
& \scriptsize {7156}
  \\ \hline
\scriptsize  {XEn}
& \scriptsize  {49}
& \scriptsize {60}
& \scriptsize {104}
& \scriptsize {196}
& \scriptsize {310}

& \scriptsize {566}
& \scriptsize {262}
& \scriptsize {1639}
  \\ \hline
\end{tabular}
\caption{Evaluation time in ASMS}\end{table}

For the three systems, we can observe that, when used conjointly with a decision engine based on XEngine rather than Sun PDP, our proposed approach 
provides more performance improvement. This empirical observation pleads in favour of applying our proposed refactoring process with XEngine as a decision engine rather than Sun PDP.\\
- Evaluation with XEngine: Taking into consideration the system load

In this step, we have considered the workload of the software system. We evaluated the request procesing time according to the number of requests incoming to the system. 
For each policy in the three systems (ASMS, LMS, and VMS), we generate successively 5000, 10000,..,50000 random requests to measure the evaluation time (ms).
The results are shown in (reference to tables). For the three systems, we notice that the evaluation time increases when the number of requests increases in the system. 
Moreover, the request evaluation time is considerably improved when using the splitting process compared to the initial architecture.
\begin{figure*}
  \centering
  \subfloat[LMS]{\label{fig:gull}\includegraphics[width=0.33\textwidth]{X-LMS.pdf}}                
  \subfloat[VMS]{\label{fig:VMS}\includegraphics[width=0.33\textwidth]{X-VMS.pdf}}
  \subfloat[ASMS]{\label{fig:ASMS}\includegraphics[width=0.33\textwidth]{X-ASMS.pdf}}
  \caption{Processing Time for our 3 systems LMS, VMS and ASMS depending on the requests Number}
  \label{fig:processing time}
\end{figure*}

We have also calculated the pourcentage of the selection of the applicable policy among the overall policies. The percentage fetching time with regards to the 
global time of request evaluation in LMS system is presented in Figure \ref{Fetching Time}.
  

\begin{figure}[!h]
  \centering
\includegraphics[width=8.5cm, height=7.2cm]{fetching.pdf}
\begin{center}
\caption{Percentage of Fetching Time}
\label{Fetching Time}
\end{center}
\end{figure} 
