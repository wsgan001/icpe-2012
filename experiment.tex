%\section{Experiment}\label{sec:experiment}
\section{Evaluation} \label{sec:experiment}

We carried out our evaluation on a desktop PC, running Ubuntu 10.04 with Core i5, 2530 Mhz processor, and 4 GB of RAM. 
We have implemented a tool, called \CodeIn{PolicySplitter} to split the policies according to given splitting criteria automatically.
The tool is implemented in Java and is available for download from \cite{splitter}.
To measure the efficiency of our approach, we conducted evaluation as follows. 
We compared request processing time with a single global policy (handled by a single PDP) and policies splitted by each splitting criterion.
We first conduct an evaluation on subjects (including PEPs and SUN PDPs) to evaluate the performance improvement in terms of decision making processing. \CodeIn{IA} denotes an ``Initial Architecture'', which uses the single global policy for request processing time.
We next conduct an evaluation on modified subjects, which replace SUN PDPs with a novel request evaluation engine, XEngine \cite{Xengine}. The objective of this evaluation is to investigate how our approach impacts performance for subjects combined with XEngine.


%and how this impacts the performance. The first subsection introduces our empirical studies and presents the tool that supports our approach. The remaining two 
%sections present and discuss the results of the two empirical studies.
%---
% We choose to use Sun XACML PDP instead of XEngine in order 
%to prove the effectiveness of our approach when compared to the traditional architecture.
%The execution time of the tool is not considered as a performance factor as it takes up to few seconds (for very large policies) to perform the splitting 
%according to all SCs. Moreover the refactoring process is executed only once to create a configuration that supports a selected splitting criterion.
%---

  

\subsection{Subjects}
The subjects include three real-life Java programs each which
interact with access control policies \cite{testcase}. We next describe
our three subjects.
\begin{itemize}	
\item Library Management System (LMS) provides web services to manage books in a public library.
\item Virtual Meeting System (VMS) provides web conference services. VMS allows users to organize
online meetings in a distributed platform.
\item Auction Sale Management System (ASMS) allows users to buy or sell items on line. A seller initiates an auction by submitting a description of an item she wants to sell with its expected minimum price. Users then participate in bidding process by
bidding the item. For the bidding on the item, users have enough money in her account before bidding.
\end{itemize}

Our subjects are initially equipped with Sun PDP \cite{sunxacml}, which is
a popularly used PDP to evaluate requests.
Policies in LMS,VMS, and ASMS contain a total of 720, 945, and 1760 rules, respectively.
%These rules include both explicit and implicit rules. Explicit rules are described
%explicitly in rules as shown at the first rule in Figure~\ref{figur1}.
%Implicit rules are not described explicitly, however can be inferred from existing explicit
%rules. 
Moreover, to compare performance improvement over existing PDPs,
we adopt XEngine (instead of Sun PDP) in our subjects to evaluate requests.
XEngine is a novel policy evaluation engine, which transforms the hierarchical tree structure of the XACML policy to a flat structure
to improve request processing time. XEngine also handles various combining algorithms supported by XACML.
 


\subsection{Objectives and Measures}
In the evaluation, we intend to answer the following
research questions:
\begin{enumerate}
	\item RQ1: Can our approach preserve policy behaviors of original subjects after refactoring?
This question helps to show that our
approach does not alter policy behaviors after refactoring based on splitting criteria.

	\item RQ2: How faster request processing time of multiple Sun PDPs with policies splitted by our approach compared
to an existing single Sun PDP? This question helps to show that our
approach can improve performance in terms of request processing time. Moreover, we
compare request processing time for different splitting criteria.

	\item RQ3:  How faster request processing time of XEngine compared
to that of Sun PDP for both a single policy and policies splitted by our approach.
This question helps to show that our approach can improve performance in terms of request processing time
for other policy evaluation engines such as XEngine.
\end{enumerate}
In our evaluation, we measure request processing time by evaluating randomly
generated requests developed by our previous work~\cite{Xengine}.
In particular, for multiple PDPs, our approach fetches a PEP with a corresponding
PDP for a given request at run time. Therefore, request processing time includes
both fetching time and request evaluation time.


\begin{figure*}[h!]
  \centering
  \subfloat[LMS]{\label{fig:gull}\includegraphics[width=0.33\textwidth]{LMS.pdf}}                
  \subfloat[VMS]{\label{fig:VMS}\includegraphics[width=0.33\textwidth]{VMS.pdf}}
  \subfloat[ASMS]{\label{fig:ASMS}\includegraphics[width=0.33\textwidth]{ASMS.pdf}}
  \caption{Request Processing Time for our subjects LMS, VMS and ASMS}
  \label{fig:processing time}
\end{figure*}

\subsection{Performance Improvement Results}\label{subsec:performanceimprovement}

We generated the resulting sub-policies for all the splitting criteria defined in Section~\ref{subsec:SplittingCriteria}.
For each splitting criteria, we have conducted systems tests to generate requests that trigger all the PEPs in the evaluation. 
The test generation step leads to the execution of all combination of possible requests described in our previous work \cite{testcase}.  
The process of tests generation is repeated for ten times in order to avoid the impact of randomness.
We applied this process to each splitting criterion and calculated evaluation time on average of a system under tests.


% We only consider the execution time of the PDP and we do not include the executions of the system functions. 
Figure \ref{fig:processing time} shows our evaluation results. Our results show that evaluation time for policies splitted based on each splitting criterion and the global policy of subjects.
In order to answer RQ1, we compared decisions against the PDP in \CodeIn{IA} and multiple PDPs with splitted policies.
consistent with our expected results.

In order to answer RQ2, from the results in Figure \ref{fig:processing time}, 
we observed the followings:

\begin{itemize} 

\item Compared to evaluation time of \CodeIn{IA}, our approach improves performance for all of splitting criteria
in terms of evaluation time. This observation is consistent with our expected results; evaluation time against
policies with smaller number of rules (compared with the number of rules in \CodeIn{IA}) is faster than that against
policies in \CodeIn{IA}.

%initial intuition. 
%In fact, splitting the policy into small policies improves requests processing duration.

\item The splitting criteria \normalsize $SC=\langle Action, Resource\rangle$ enables to show the lowest evaluation time. 
Recall that the PEPs in the evaluation are scattered across different methods in a subject by a categorization 
that is based on $SC_{2}=\langle Resource,Action\rangle$. This observation pleads in favor of applying a splitting criteria 
that takes into account the PEP-PDP synergy and show the best performance.
%This splitting preserves the initial architecture by keeping a strong mapping between the PEPS and the PDP and enables 
%to have the best results in terms of performance.



\end{itemize} 

\begin{figure}[!h]
  \centering
\includegraphics[width=8.5cm, height=7.2cm]{pdpnumber.pdf}
\begin{center}
\caption{PDP Number According to Splitting Criteria}
\label{pdpnumber}
\end{center}
\end{figure}
Figure \ref{pdpnumber} shows the number of policies splitted by our approach. 
We observed the number of policies based on our proposed three categories: (1) $SC_{1}$ category leads to the smallest number $N_1$ of PDPs, (2) $SC_{2}$ category produces a reasonable number $N_2$ ($N_1$<$N_2$<$N_3$) of PDPs, and (3) $SC_{3}$ leads to the largest number $N_3$ of PDPs.
While $SC_{1}$ category leads to the smallest number of PDPs, each PDP encapsulates a relatively high number of rules in a policy (compared
with that of $SC_{2}$ and $SC_{3}$, which leads to performance degrade. We observed that $SC_{3}$ category leads to the largest number of PDPs, which takes higher fetching time (compared with that of $SC_{1}$ and $SC_{2}$). 
We observed that, in our evaluation, $SC=\langle Action, Resource\rangle$ is the best criterion, both in terms of performances and relatively low number of PDPs.


%\subsubsection{Performance improvement with XEngine}


\begin{table}[t]
\centering
\begin{tabular}{|>{\tiny}c|>{\tiny}c|>{\tiny}c|>{\tiny}c|>{\tiny}c|>{\tiny}c|>{\tiny}c|>{\tiny}c|>{\tiny}c|}   
\hline  \rowcolor{black} \scriptsize \bf \textcolor {white}{}
& \scriptsize \bf \textcolor {white}{SAR}
& \scriptsize \bf \textcolor {white}{AR}
& \scriptsize \bf \textcolor  {white}{SA}
& \scriptsize \bf \textcolor  {white}{SR}
& \scriptsize \bf \textcolor  {white}{R}

& \scriptsize \bf \textcolor  {white}{S} 
& \scriptsize \bf \textcolor  {white}{A}
& \scriptsize \bf \textcolor {white}{IA}\\ \hline
\scriptsize  {SUN }
&\scriptsize  {485}
& \scriptsize {922}
& \scriptsize {1453}
& \scriptsize {1875}
& \scriptsize {2578}

& \scriptsize {2703}
& \scriptsize {2703}
& \scriptsize {2625}
  \\ \hline
\scriptsize  {XEn}
&\scriptsize  {26}
& \scriptsize {47}
& \scriptsize {67}
& \scriptsize {95}
& \scriptsize {190}

& \scriptsize {164}
& \scriptsize {120}
& \scriptsize {613}
  \\ \hline
\end{tabular}

\caption{Evaluation time in LMS}
\label{table:LMSeval}
\vspace{5 mm}
%\end{table}
%\begin{table}[h!]
\centering
\begin{tabular}{|l|l|l|l|l|l|l|l|l|}   
\hline  \rowcolor{black} \scriptsize \bf \textcolor {white}{}
& \scriptsize \bf \textcolor {white}{SAR}
& \scriptsize \bf \textcolor {white}{AR}
& \scriptsize \bf \textcolor  {white}{SA}
& \scriptsize \bf \textcolor  {white}{SR}
& \scriptsize \bf \textcolor  {white}{R}

& \scriptsize \bf \textcolor  {white}{S} 
& \scriptsize \bf \textcolor  {white}{A}

& \scriptsize \bf \textcolor {white}{IA}\\ \hline

\scriptsize  {SUN }
& \scriptsize  {1281}
& \scriptsize {2640}
& \scriptsize {3422}
& \scriptsize {3734}
& \scriptsize {6078}

& \scriptsize {5921}
& \scriptsize {6781}
& \scriptsize {5766}
  \\ \hline
\scriptsize  {XEn}
& \scriptsize  {34}
& \scriptsize {67}
& \scriptsize {96}
& \scriptsize {145}
& \scriptsize {384}

& \scriptsize {274}
& \scriptsize {149}
& \scriptsize {265}
  \\ \hline
\end{tabular}

\caption{Evaluation time in VMS}
\label{table:VMSeval}
\vspace{5 mm}
%\end{table}
%
%\begin{table}[h!]
\centering
\begin{tabular}{|l|l|l|l|l|l|l|l|l|}   
\hline  \rowcolor{black} \scriptsize \bf \textcolor {white}{}
& \scriptsize \bf \textcolor {white}{SAR}
& \scriptsize \bf \textcolor {white}{AR}
& \scriptsize \bf \textcolor  {white}{SA}
& \scriptsize \bf \textcolor  {white}{SR}
& \scriptsize \bf \textcolor  {white}{R}

& \scriptsize \bf \textcolor  {white}{S} 
& \scriptsize \bf \textcolor  {white}{A}
& \scriptsize \bf \textcolor {white}{IA}\\ \hline
\scriptsize  {SUN }
& \scriptsize  {2280}
& \scriptsize {2734}
& \scriptsize {3625}
& \scriptsize {8297}
& \scriptsize {7750}

& \scriptsize {8188}
& \scriptsize {6859}
& \scriptsize {7156}
  \\ \hline
\scriptsize  {XEn}
& \scriptsize  {49}
& \scriptsize {60}
& \scriptsize {104}
& \scriptsize {196}
& \scriptsize {310}

& \scriptsize {566}
& \scriptsize {262}
& \scriptsize {1639}
  \\ \hline
\end{tabular}

\caption{Evaluation time in ASMS}
\label{table:ASMSeval}
\end{table}

In order to answer RQ3, we measurer request processing time of XEngine compared with that of Sun PDP
for policies splitted by our approach.
We generated 10,000 random requests proposed in our previous work \cite{request} and measured request processing time (in ms).
Tables~\ref{table:LMSeval},~\ref{table:VMSeval}, and~\ref{table:ASMSeval} show our evaluation results.
We observed that, in cases where our subjects equipped with XEngine instead of Sun PDP, our proposed approach 
improves performance (compared that of Sun PDP). This observation pleads in favour of applying our proposed refactoring process with XEngine as a decision instead of Sun PDP.\\

%- Evaluation with XEngine: Taking into consideration the system load
We next evaluated request processing time according to the number of requests incoming to the system. 
For each policy in the three systems (ASMS, LMS, and VMS), we generated 5000, 10000, .., 50000 random requests to measure the evaluation time (ms).
The results are shown in (reference to tables). For the three systems, we notice that the evaluation time increases when the number of requests increases in the system. 
Moreover, the request evaluation time is considerably improved when using the splitting process compared to the initial architecture.

\begin{figure*}
  \centering
  \subfloat[LMS]{\label{fig:gull}\includegraphics[width=0.33\textwidth]{X-LMS.pdf}}                
  \subfloat[VMS]{\label{fig:VMS}\includegraphics[width=0.33\textwidth]{X-VMS.pdf}}
  \subfloat[ASMS]{\label{fig:ASMS}\includegraphics[width=0.33\textwidth]{X-ASMS.pdf}}
  \caption{Processing Time for our subjects, LMS, VMS and ASMS depending on the requests Number}
  \label{fig:processing time}
\end{figure*}

Figure \ref{Fetching Time} shows percentage of fetching time over overall evaluation time for request evaluation in LMS. Our
results show that feting time is relatively small in comparison with a total evaluation time.
  

\begin{figure}[!h]
  \centering
\includegraphics[width=8.5cm, height=7.2cm]{fetching.pdf}
\begin{center}
\caption{Percentage of Fetching Time}
\label{Fetching Time}
\end{center}
\end{figure} 


%--------------------------------------Comment Out---------------------------------------
\Comment{
%\section{Experiment}\label{sec:experiment}
\section{Empirical results} \label{sec:experiment}
To measure the efficiency of our approach, we conducted two empirical studies. The first one takes into consideration the whole system 
(PEPs and PDPs) to evaluate the performance improvement regarding the decision making process. The request processing time, 
for each splitting criterion is compared to the processing time of the initial architecture implementing the global policy (the evaluation 
that considers the global policy, is denoted (IA), the ``Initial Architecture''). The second empirical study focuses only on the PDPs in isolation to measure the gain in performance
 independently from the system. To make such study of PDPs in isolation, we use XEngine \cite{Xengine}. The objective of the second study is to see how our approach can be 
combined with XEngine and how this impacts the performance. The first subsection introduces our empirical studies and presents the tool that supports our approach. The remaining two 
sections present and discuss the results of the two empirical studies.  

\subsection{Empirical Studies and PolicySplitter Tool}
The empirical studies were conducted using the following systems \cite{testcase}:
\begin{itemize}	
\item LMS: The library management system offers services to manage books in a public library.
\item VMS: The virtual meeting system offers simplified web conference services. The virtual meeting server allows the organization of work meetings on 
a distributed platform.
\item ASMS (Auction Sale Management System): allows users to buy or sell items online. A seller can start an auction by submitting a description of the
item he wants to sell and a minimum price (with a start date and an ending date for the auction). Then usual bidding process can apply and people can bid 
on this auction. One of the specificities of this system is that a buyer must have enough money in his account before bidding.
\end{itemize}
We completed the set of existing rules with all implicit rules that were not explicitely originally described. This provides a larger set of access rules that enables
an accurate comparison of performances. As a result, LMS policy contains 720 rules, VMS has 945 rules while ASMS implements 1760 rules. 
Our evaluations were carried out on a desktop PC, running Ubuntu 10.04 with Core i5, 2530 Mhz processor, and 4 GB of RAM. 
We have implemented a tool to automate policies refactoring. PolicySplitter enables splitting the policies according to the chosen splitting criteria. 
The tool is implemented using the Java language and is available for download from \cite{splitter}.
The execution time of the tool is not considered as a performance factor as it takes up to few seconds (for very large policies) to perform the splitting 
according to all SCs. Moreover the refactoring process is executed only once to create a configuration that supports a selected splitting criterion.


\subsection{Performance Improvement Results}
For the 3 evaluation studies, we generated the resulting sub-policies for all the splitting criteria that we have defined in Section III.
The decision Engine in our three case studies is based on Sun XACML PDP implementation \cite{sunxacml}. We choose to use Sun XACML PDP instead of XEngine in order 
to prove the effectiveness of our approach when compared to the traditional architecture.
% To run the experiments, we used a configuartion file that maintains the list of policies used for requests evaluation. 
% For the initial architecture, the configuration file contains one policy. Sun XACML PDP uses this configuration file to select to XACML files that constitute the policy.
For each splitting criteria, we have conducted systems tests to generate requests that trigger all the PEPs in the three evaluation studies. 
The test generation step leads to the execution of all combination of possible requests, all the details related to this step are presented 
in details in our previous \cite{testcase}.  
The process of tests generation is repeated ten times in order to avoid the impact of randomness.
We applied this process to each splitting criterion and calculated the average execution time of the system under tests.


% We only consider the execution time of the PDP and we do not include the executions of the system functions. 
The results are shown in Figure \ref{fig:processing time}. They show the execution time considering the sub-policies resulting from each splitting criterion and the global policy 
that corresponds to the initial architecture (IA). Note that the results are ranked from the largest processing duration to the smallest one. 
Through the results shown in Figure \ref{fig:processing time}, we can make two observations:

\begin{itemize} 
\item Compared to the initial architecture (IA), the evaluation time is considerably reduced for all the splitting criteria. This is consistent with our 
initial intuition. In fact, splitting the policy into small policies improves requests processing duration.


\item The splitting criteria \normalsize $SC=\langle Action, Resource\rangle$ enables to have the best evaluation time. 
recall that the PEPs in the 3 empirical studies are scattered across the applications by a categorization 
that is based on $SC_{2}=\langle Resource,Action\rangle$. This pleads in favor of applying a splitting criteria 
that takes into account the PEP-PDP synergy. This splitting preserves the initial architecture by keeping a strog mapping between the PEPS and the PDP and enables 
to have the best results in terms of performance.

\end{itemize} 
\begin{figure*}
  \centering
  \subfloat[LMS]{\label{fig:gull}\includegraphics[width=0.33\textwidth]{LMS.pdf}}                
  \subfloat[VMS]{\label{fig:VMS}\includegraphics[width=0.33\textwidth]{VMS.pdf}}
  \subfloat[ASMS]{\label{fig:ASMS}\includegraphics[width=0.33\textwidth]{ASMS.pdf}}
  \caption{Processing Time for our 3 systems LMS, VMS and ASMS}
  \label{fig:processing time}
\end{figure*}
\begin{figure}[!h]
  \centering
\includegraphics[width=8.5cm, height=7.2cm]{pdpnumber.pdf}
\begin{center}
\caption{PDP Number According to Splitting Criteria}
\label{pdpnumber}
\end{center}
\end{figure} 

We have evaluated the PDPs number generated by each splitting criterion, to study the impact of the refactoring process on the initial policy. 
For the 3 XACML studies, we executed the PoliySplitter tool on the 3 initial policies and we generated the number of resulting policies, in each study. 
As highlighted by Figure \ref{pdpnumber}, we notice that there are three
 categories of results: $SC_{1}$ category leads to a small number of PDPs, $SC_{2}$ category produces a reasonable number of PDPs whereas $SC_{3}$ leads to a huge 
number of PDPs. $SC_{2}$ category, is a good tradeoff in terms of performance and number of 
PDPs generated: In our evaluation studies, $SC=\langle Action, Resource\rangle$ is the best criterion, both in terms of performances and low number of PDPs.


\begin{table}[h!]
\centering
\begin{tabular}{|>{\tiny}c|>{\tiny}c|>{\tiny}c|>{\tiny}c|>{\tiny}c|>{\tiny}c|>{\tiny}c|>{\tiny}c|>{\tiny}c|}   
\hline  \rowcolor{black} \scriptsize \bf \textcolor {white}{}
& \scriptsize \bf \textcolor {white}{S}
& \scriptsize \bf \textcolor {white}{A}
& \scriptsize \bf \textcolor  {white}{R}
& \scriptsize \bf \textcolor  {white}{SA}
& \scriptsize \bf \textcolor  {white}{SR}
& \scriptsize \bf \textcolor  {white}{AR} 
& \scriptsize \bf \textcolor  {white}{SAR}
& \scriptsize \bf \textcolor {white}{IA}\\ \hline


\scriptsize  {LMS}
&\scriptsize  {120}
& \scriptsize {72}
& \scriptsize {240}
& \scriptsize {12}
& \scriptsize {40}
& \scriptsize {24}
& \scriptsize {4}
& \scriptsize {720}
  \\ \hline


\scriptsize  {VMS}
&\scriptsize  {135}
& \scriptsize {63}
& \scriptsize {315}
& \scriptsize {9}
& \scriptsize {45}
& \scriptsize {21}
& \scriptsize {3}
& \scriptsize {945}
  \\ \hline
\scriptsize  {ASMS}
&\scriptsize  {220}
& \scriptsize {160}
& \scriptsize {352}
& \scriptsize {20}
& \scriptsize {44}
& \scriptsize {32}
& \scriptsize {4}
& \scriptsize {1760}
  \\ \hline
\scriptsize  {Synergic}
&\scriptsize  {}
& \scriptsize {x}
& \scriptsize {x}
& \scriptsize {}
& \scriptsize {}
& \scriptsize {x}
& \scriptsize {}
& \scriptsize {x}
  \\ \hline

\scriptsize  {Not Synergic}
&\scriptsize  {x}
& \scriptsize {}
& \scriptsize {}
& \scriptsize {x}
& \scriptsize {x}
& \scriptsize {}
& \scriptsize {x}
& \scriptsize {}
  \\ \hline
\end{tabular}
\caption{Average of rules number/PDP in the 3 systems}\end{table}

\subsubsection{Performance improvement with XEngine}
The goal of this empirical study is to show the impact of combining XEngine as a decision engine rather than Sun XACML PDP implementation with our approach. 
In this step, we show the effectiveness of the refactoring process independently from the appication code.
We have chosen to use \cite{Xengine}, mainly for 3 reasons:

\begin{itemize}
\item It uses a refactoring process that transforms the hierarchical structure of the XACML policy to a flat structure. 
\item It converts multiple combining algorithms to single one.
\item It lies on a tree structure that minimizes the request processing time.
\end{itemize}
We propose to use XEngine conjointly with the refactoring process presented in this work:
We have evaluated our approach in 2 settings:
\begin{itemize}
\item Considering evaluations with a decision engine, based on SUN PDP, with split policies and with the initial policy.  
\item Considering evaluations with a decision engine, based on XEngine rather than Sun PDP, with split policies and with the initial policy as well.  
\end{itemize}

This step consists in two parts, first, we consider to measure the request evaluation time for a fixed number of request, then we measure the request evaluation
 time by taking into consideration the load of the overall system.


- Using XEngine with a fixed number of requests
We have measured the processing time (in ms) of a randomly generated set of 10,000 requests. For request generation, we have used the technique presented 
in \cite{request}. The request time processing is evaluated for LMS, VMS, ASMS. The results are presented in Table I, II and III.

\begin{table}[h!]
\centering
\begin{tabular}{|>{\tiny}c|>{\tiny}c|>{\tiny}c|>{\tiny}c|>{\tiny}c|>{\tiny}c|>{\tiny}c|>{\tiny}c|>{\tiny}c|}   
\hline  \rowcolor{black} \scriptsize \bf \textcolor {white}{}
& \scriptsize \bf \textcolor {white}{SAR}
& \scriptsize \bf \textcolor {white}{AR}
& \scriptsize \bf \textcolor  {white}{SA}
& \scriptsize \bf \textcolor  {white}{SR}
& \scriptsize \bf \textcolor  {white}{R}

& \scriptsize \bf \textcolor  {white}{S} 
& \scriptsize \bf \textcolor  {white}{A}
& \scriptsize \bf \textcolor {white}{IA}\\ \hline
\scriptsize  {SUN }
&\scriptsize  {485}
& \scriptsize {922}
& \scriptsize {1453}
& \scriptsize {1875}
& \scriptsize {2578}

& \scriptsize {2703}
& \scriptsize {2703}
& \scriptsize {2625}
  \\ \hline
\scriptsize  {XEn}
&\scriptsize  {26}
& \scriptsize {47}
& \scriptsize {67}
& \scriptsize {95}
& \scriptsize {190}

& \scriptsize {164}
& \scriptsize {120}
& \scriptsize {613}
  \\ \hline
\end{tabular}
\caption{Evaluation time in LMS}\end{table}



\begin{table}[h!]
\centering
\begin{tabular}{|l|l|l|l|l|l|l|l|l|}   
\hline  \rowcolor{black} \scriptsize \bf \textcolor {white}{}
& \scriptsize \bf \textcolor {white}{SAR}
& \scriptsize \bf \textcolor {white}{AR}
& \scriptsize \bf \textcolor  {white}{SA}
& \scriptsize \bf \textcolor  {white}{SR}
& \scriptsize \bf \textcolor  {white}{R}

& \scriptsize \bf \textcolor  {white}{S} 
& \scriptsize \bf \textcolor  {white}{A}

& \scriptsize \bf \textcolor {white}{IA}\\ \hline

\scriptsize  {SUN }
& \scriptsize  {1281}
& \scriptsize {2640}
& \scriptsize {3422}
& \scriptsize {3734}
& \scriptsize {6078}

& \scriptsize {5921}
& \scriptsize {6781}
& \scriptsize {5766}
  \\ \hline
\scriptsize  {XEn}
& \scriptsize  {34}
& \scriptsize {67}
& \scriptsize {96}
& \scriptsize {145}
& \scriptsize {384}

& \scriptsize {274}
& \scriptsize {149}
& \scriptsize {265}
  \\ \hline
\end{tabular}
\caption{Evaluation time in VMS}\end{table}

\begin{table}[h!]
\centering
\begin{tabular}{|l|l|l|l|l|l|l|l|l|}   
\hline  \rowcolor{black} \scriptsize \bf \textcolor {white}{}
& \scriptsize \bf \textcolor {white}{SAR}
& \scriptsize \bf \textcolor {white}{AR}
& \scriptsize \bf \textcolor  {white}{SA}
& \scriptsize \bf \textcolor  {white}{SR}
& \scriptsize \bf \textcolor  {white}{R}

& \scriptsize \bf \textcolor  {white}{S} 
& \scriptsize \bf \textcolor  {white}{A}
& \scriptsize \bf \textcolor {white}{IA}\\ \hline
\scriptsize  {SUN }
& \scriptsize  {2280}
& \scriptsize {2734}
& \scriptsize {3625}
& \scriptsize {8297}
& \scriptsize {7750}

& \scriptsize {8188}
& \scriptsize {6859}
& \scriptsize {7156}
  \\ \hline
\scriptsize  {XEn}
& \scriptsize  {49}
& \scriptsize {60}
& \scriptsize {104}
& \scriptsize {196}
& \scriptsize {310}

& \scriptsize {566}
& \scriptsize {262}
& \scriptsize {1639}
  \\ \hline
\end{tabular}
\caption{Evaluation time in ASMS}\end{table}

For the three systems, we can observe that, when used conjointly with a decision engine based on XEngine rather than Sun PDP, our proposed approach 
provides more performance improvement. This empirical observation pleads in favour of applying our proposed refactoring process with XEngine as a decision engine rather than Sun PDP.\\
- Evaluation with XEngine: Taking into consideration the system load

In this step, we have considered the workload of the software system. We evaluated the request procesing time according to the number of requests incoming to the system. 
For each policy in the three systems (ASMS, LMS, and VMS), we generate successively 5000, 10000,..,50000 random requests to measure the evaluation time (ms).
The results are shown in (reference to tables). For the three systems, we notice that the evaluation time increases when the number of requests increases in the system. 
Moreover, the request evaluation time is considerably improved when using the splitting process compared to the initial architecture.
\begin{figure*}
  \centering
  \subfloat[LMS]{\label{fig:gull}\includegraphics[width=0.33\textwidth]{X-LMS.pdf}}                
  \subfloat[VMS]{\label{fig:VMS}\includegraphics[width=0.33\textwidth]{X-VMS.pdf}}
  \subfloat[ASMS]{\label{fig:ASMS}\includegraphics[width=0.33\textwidth]{X-ASMS.pdf}}
  \caption{Processing Time for our 3 systems LMS, VMS and ASMS depending on the requests Number}
  \label{fig:processing time}
\end{figure*}

We have also calculated the pourcentage of the selection of the applicable policy among the overall policies. The percentage fetching time with regards to the 
global time of request evaluation in LMS system is presented in Figure \ref{Fetching Time}.
  

\begin{figure}[!h]
  \centering
\includegraphics[width=8.5cm, height=7.2cm]{fetching.pdf}
\begin{center}
\caption{Percentage of Fetching Time}
\label{Fetching Time}
\end{center}
\end{figure}
} 
